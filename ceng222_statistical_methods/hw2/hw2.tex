\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{float}
\usepackage{amsmath}


\usepackage[hmargin=3cm,vmargin=6.0cm]{geometry}
\topmargin=-2cm
\addtolength{\textheight}{6.5cm}
\addtolength{\textwidth}{2.0cm}
\setlength{\oddsidemargin}{0.0cm}
\setlength{\evensidemargin}{0.0cm}
\usepackage{indentfirst}
\usepackage{amsfonts}

\begin{document}

\section*{Student Information}

Name : Mustafa Alperen Bitirgen\\

ID : 2231496\\

\section*{Answer 1}
\text{\hspace*{-0.6cm}Before getting started we need to recall a definition. To get the marginal pmf of one variable,}\\
\text{we add the joint probabilities over all values of the other variable.}\\
\[P_{X}(x) = P\{X = x\} = \sum_{y} P_{(X,Y)}(x, y)\]
\subsection*{a)}
\[P_{X}(0) = P\{X = 0\} = \sum_{y} P_{(X,Y)}(0, y) = P_{(X,Y)}(0, 0) + P_{(X,Y)}(0, 2) = \frac{1}{12} + \frac{2}{12} = \frac{3}{12}\]
\[P_{X}(1) = P\{X = 1\} = \sum_{y} P_{(X,Y)}(1, y) = P_{(X,Y)}(1, 0) + P_{(X,Y)}(1, 2) = \frac{4}{12} + \frac{2}{12} = \frac{6}{12}\]
\[P_{X}(2) = P\{X = 2\} = \sum_{y} P_{(X,Y)}(2, y) = P_{(X,Y)}(2, 0) + P_{(X,Y)}(2, 2) = \frac{1}{12} + \frac{2}{12} = \frac{3}{12}\]
\vspace*{0.4cm}
\[\mu_X = E(X) = \sum_{x} xP(x) = 0\times\frac{3}{12} + 1\times\frac{6}{12} + 2\times\frac{3}{12} = 1\]
\[Var(X) = E(X - E(X))^2 = \sum_{x} (x - \mu_X)^2 P(x) = (0-1)^2 \times\frac{3}{12} + (1-1)^2 \times\frac{6}{12} + (2-1)^2 \times\frac{3}{12} = \frac{6}{12} = 0.5\]
\subsection*{b)}
\text{\hspace*{-0.6cm}Let's call the probability mass function Z, such that Z = X + Y.}\\
\[P_z(0) = P\{X + Y = 0\}  = P_{(X,Y)}(0, 0) = \frac{1}{12}\]
\[P_z(1) = P\{X + Y = 1\}  = P_{(X,Y)}(1, 0) = \frac{4}{12}\]
\[P_z(2) = P\{X + Y = 2\}  = P_{(X,Y)}(0, 2) + P_{(X,Y)}(2, 0) = \frac{2}{12} + \frac{1}{12} = \frac{3}{12}\]
\[P_z(3) = P\{X + Y = 3\}  = P_{(X,Y)}(1, 2) = \frac{2}{12}\]
\[P_z(4) = P\{X + Y = 4\}  = P_{(X,Y)}(2, 2) = \frac{2}{12}\]
\subsection*{c)}
\[Cov(X, Y) = E \{(X \text{ - } E(X))(Y \text{ - } E(Y))\} = E(XY) \text{ - } E(X)E(Y)\]
\text{Before we determine Cov(X,Y), we first need to do some calculations.}\\
\vspace*{0.4cm}
\[P_{Y}(0) = P\{Y = 0\} = \sum_{x} P_{(X,Y)}(x, 0) = P_{(X,Y)}(0, 0) + P_{(X,Y)}(1, 0) + P_{(X,Y)}(2, 0) = \frac{1}{12} + \frac{4}{12} + \frac{1}{12} = \frac{6}{12}\]
\[P_{Y}(2) = P\{Y = 2\} = \sum_{x} P_{(X,Y)}(x, 2) = P_{(X,Y)}(0, 2) + P_{(X,Y)}(1, 2) + P_{(X,Y)}(2, 2) = \frac{2}{12} + \frac{2}{12} + \frac{2}{12} = \frac{6}{12}\]
\[\hspace*{-2cm}\mu_{Y} = E(Y) = \sum_{y} yP(y) = 0\times\frac{6}{12} + 2\times\frac{6}{12} = 1\]
\[\hspace*{-4cm}\mu_{XY} = E(XY) = \sum_{x}\sum_{y} xyP_{(X,Y)}(x, y) \]
\[\hspace*{2cm} = (1)(2)P_{(X,Y)}(1, 2) + (2)(2)P_{(X,Y)}(2, 2) = 2\times \frac{2}{12} + 4\times \frac{2}{12} = 1\]\\
\vspace*{.4cm}
\[Cov(X, Y)  = E(XY) \text{ - } E(X)E(Y) = \mu_{XY} - \mu_{X}\mu_{Y} = 1 \text{ - } 1\times1 = 0\]\\
\subsection*{d)}
\[\mu_A = E(A) = \sum_{a} aP(a)\]
\[\mu_B = E(B) = \sum_{b} bP(b)\]\\
\[Cov(A, B)  = E[(A - E(A))(B - E(B))] = E[AB - \mu_{A}B - \mu_{B}A + \mu_{A}\mu_{B}]\]
\[\hspace*{-2cm} = E[AB] - \mu_{A}E(B) - \mu_{B}E(A) + \mu_{A}\mu_{B}\]
\[\hspace*{-2.9cm} = E[AB] - \mu_{A}\mu_{B} - \mu_{B}\mu_{A} + \mu_{A}\mu_{B}\]
\[\hspace*{-5.9cm} = E[AB] - \mu_{A}\mu_{B}\]\\
\vspace*{.4cm}\\
\text{Then we need to find an expression for the expectation of AB, such that E(AB):}\\
\[\hspace*{-5.9cm} E[AB] = \sum_{a}\sum_{b} abP_{(A,B)}(a, b)\]
\text{If the random variables A and B are independent of each other we can rewrite as: }\\
\[\hspace*{-5.9cm} E[AB] = \sum_{a}\sum_{b} abP_{A}(a)P_{B}(b)\]
\[\hspace*{-4cm} = (\sum_{a} aP_{A}(a))(\sum_{b}bP_{B}(b)) \]
\[ \hspace*{-5cm} = E[A]E[B] = \mu_{A}\mu_{B}\]\\
\vspace{0.4cm}\\
\text{Then we can find the covariance of random variables A and B, such that A and B are independent}\\
\text{Cov(A,B):}\\
\[Cov(A, B)  = E[AB] - \mu_{A}\mu_{B} = \mu_{A}\mu_{B} - \mu_{A}\mu_{B} = 0 \]
\subsection*{e)}
\text{\hspace*{-0.6cm}Since the statement "If the random variables A and B are independent, Cov(A,B)" is not an if and}\\
\text{only if statement, we can not relate what we have found in part (c) to the independence of the}\\
\text{random variables X and Y. Instead, we need to check whether the following holds for all values of}\\
\text{x and y.}\\
\[P_{(X,Y)}(x,y) = P_X(x)P_Y(y)\]
\text{Let us first try the values X = 0 and Y = 0:}\\
\[P_{(X,Y)}(0,0) = \frac{1}{12} \text{, } P_X(0) = \frac{3}{12} \text{, } P_Y(0) = \frac{6}{12}\]
\[P_{(X,Y)}(0,0) \neq P_X(0)P_Y(0)\]
\text{Since 1 counter example is enough, we can clearly say that random variables X and Y are}\\
\textbf{ not independent}.\\
\section*{Answer 2}
\subsection*{a)}
\text{\hspace*{-0.6cm}By using the definition of the Binomial Distribution and defining the random variable B as the }\\
\text{number of broken pens among 12 pens:}\\
\text{B = \{Number of Broken Pens Among 12 Pens\}}\\
\[\text{P$_B$(b) = P(B = b) = $\binom{12}{b}$ $(0.2)^{b}$ $(0.8)^{12-b}$}\]\\
\[\text{P(B $\geq$ 3) = 1 - P(B $\leq$ 2) = 1 - $\sum_{k =0}^{2}$$\binom{12}{b}$ $(0.2)^{b}$ $(0.8)^{12-b}$}\]

\[\hspace*{4.5cm}= 1 - [\binom{12}{0}(0.8)^{12} + \binom{12}{1}(0.2)^{1}(0.8)^{11} + \binom{12}{2}(0.2)^{2}(0.8)^{10}]\]
\[\hspace*{2.2cm}= 1 - [(0.8)^{12} + 12(0.2)^{1}(0.8)^{11} + 66(0.2)^{2}(0.8)^{10}]\]
\[\hspace*{-2.3cm}= 1 - [0.5583] = 0.4417\]\\

\subsection*{b)}
\text{\hspace*{-0.6cm}For fifth pen we test to be second broken pen, we only have to 1 broken pen among the first 4 pens.}\\
\text{By using the definition of the Binomial Distribution and defining the random variable D as the number}\\
\text{of broken pens among 4 pens:}\\
\text{D = \{Number of Broken Pens Among 4 Pens\}}\\
\[\text{P$_D$(d) = P(D = d) = $\binom{4}{d}$ $(0.2)^{d}$ $(0.8)^{4-d}$}\]\\
\text{Then: }\\
\[\text{P$_D$(1) = P(D = 1) = $\binom{4}{1}$ $(0.2)^{1}$ $(0.8)^{3}$ = 4(0.2)$(0.8)^{3}$ = 0.4096}\]\\
\text{Since we have found the probability that there is only one broken pen among first 4, we now need to}\\
\text{multiply that with (0.2) to obtain the probability that fifth pen we test to be second broken pen:}\\
\[(0.4096)(0.2) = 0.8192\]
\subsection*{c)}
\text{\hspace*{-0.6cm}We already know the Binomial probability density function: }\\
\[P(x) = P(X = x) = \binom{n}{x}p^xq^{n-x}\]
\text{Then we need to find an expression for the expected value:}\\
\[E[X] = \sum_{x = 0}^nx\binom{n}{x}p^xq^{n-x}\]
\[\hspace*{1.2cm} = \sum_{x = 1}^nx\binom{n}{x}p^xq^{n-x}\]
\[\hspace*{2.3cm}= \sum_{x = 1}^n\frac{xn}{x}\binom{n-1}{x-1}p^xq^{n-x}\]
\[\hspace*{2.6cm} = np\sum_{x = 1}^n\binom{n-1}{x-1}p^{x-1}q^{n-x}\]
\[\hspace*{3.9cm} = np\sum_{x = 1}^n\binom{n-1}{x-1}p^{x-1}q^{(n-1)-(x-1)}\]
\[\hspace*{1.7cm} = np(p+q)^{n-1} = np\]
\text{The p, the probability of a pen is broken, is 0.2. Therefore, on average, to find 4 broken pens we}\\
\text{need to test n pens, such that:}\\
\[n = \frac{4}{p} = \frac{4}{0.2} = \textbf{20}\]
\section*{Answer 3}
\text{\hspace*{-0.6cm}From the definition we know that exponential distribution has the density and expectation:}\\
\[f(x) = \lambda e^{-\lambda x}\]
\[E[X] = \int tf(t)dt = \int_{0}^{\infty} t\lambda e^{-\lambda t}dt = \frac{1}{\lambda}\]
\text{Since Bob gets a phone call every 4 hours on average, E(X) = 4, $\lambda$ = $\frac{1}{4}$.}\\
\vspace*{0.4cm}\\
\text{Also, let us define the random variable X, where X is the number of events during the time}\\
\text{interval [0, t]. This X has Poisson distribution with parameter $\lambda$t:}\\
\[P_X(x) = e^{-\lambda t} \frac{(\lambda t)^x}{x!} \]\\
\subsection*{a)}
\text{\hspace*{-0.6cm}Probability of " Bob does not get a phone call for at least 2 hours" is equal to 1 minus the}\\
\text{probability of "Bob gets a phone call in first 2 hours".}\\
\text{The probability of getting a phone call at least once:}\\
\[P_X\{X \geq 1\} = 1 - P_X\{X = 0\} = 1 - P_X(0) = 1 - e^{-\lambda t}\frac{(\lambda t)^0}{0!} = 1 - e^{-\lambda t}\]\\
\text{The probability of getting a phone call at least once in the time interval [0,t]:}\\
\[F_T(t) = 1 - P_X\{X \geq 1\} =  e^{-\lambda t} \]\\
\text{With the $\lambda$ = $\frac{1}{4}$ and t = 2:}\\
\[F_T(2) = 1 - P_X\{X \geq 1\} =  e^{-\frac{1}{2}} = 0.6065\]\\
\vspace*{0.3cm}\\
\textbf{Alternatively:}\\
\[\int_{2}^{\infty} \lambda e^{-\lambda t}dt = [1 - e^{-\lambda t}]_2^\infty \xrightarrow{For \lambda = \frac{1}{4}} [1 - e^{\frac{t}{4}}]_2^\infty = e^{-\frac{1}{2}} = 0.6065\]
\subsection*{b)}
\text{\hspace*{-0.6cm}The probability of getting 3 phone calls at most:}\\
\[P_X\{X \leq 3\} = P_X\{X = 0\} + P_X\{X = 1\} + P_X\{X = 2\} + P_X\{X = 3\}\]\\
\[\hspace*{-0.9cm} = e^{-\lambda t} + \lambda t e^{-\lambda t} + \frac{(\lambda t)^2}{2} e^{-\lambda t} + \frac{(\lambda t)^3}{6} e^{-\lambda t} \]\\
\[\hspace*{-2.4cm} = \frac{e^{-\lambda t}}{6}(6 + 6\lambda t + 3 \lambda^2 t^2 + \lambda^3t^3)\]\\
\text{The probability of getting 3 phone calls at most in the time interval [0,t]:}\\
\[F_T(t) = P_X\{X \leq 3\} =  \frac{e^{-\lambda t}}{6}(6 + 6\lambda t + 3 \lambda^2 t^2 + \lambda^3t^3) \]\\
\text{With the $\lambda$ = $\frac{1}{4}$ and t = 10:}\\
\[F_T(10) = P_X\{X \leq 3\} =  \frac{443 e^{-2.5}}{48} = 0.7575 \]\\
\subsection*{c)}
\text{\hspace*{-0.6cm}Here, let us define two random variables K and M such that:}\\
\text{K = "Getting 3 phone calls at most in first 16 hours"}\\
\text{M = "Getting 3 phone calls at most in first 10 hours"}\\
\vspace*{0.4cm}\\
\text{And we are asked to find, given M the probability of K, such that: }\\
\[P(K|M) = \frac{P(K \cap M)}{P(M)}\]\\
\text{Here we can clearly see that:}\\
\[P(K \cap M) = P(K)\]\\
\text{Therefore, our equation becomes:}\\
\[P(K|M) = \frac{P(K)}{P(M)}\]
\text{We have found the probability of getting 3 phone calls at most at part (b), now we use that probability}\\
\text{individually for the events K and M with $\lambda$ = $\frac{1}{4}$ and t is equal to 16 and 10 respectively. }\\
\[F_T(10) = P_X\{X \leq 3\} =  \frac{443 e^{-2.5}}{48} = 0.7575 \]\\
\[F_T(16) = P_X\{X \leq 3\} =  \frac{142 e^{-4}}{6} = 0.4335 \]\\
\[P(K|M) = \frac{P(K)}{P(M)} = \frac{F_T(16)}{F_T(10)} = \frac{0.4335}{0.7575} = 0.5722\]
\end{document}


